# large-scale-MLSys
These are MLSys reading list for private use

## Paper List of Model Parallelism
1. Automatic Graph Partitioning for Very Large-scale Deep Learning [IPDPS'21](https://arxiv.org/abs/2103.16063)
2. GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism [[NIPS'19]https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf][[code]https://github.com/tensorflow/lingvo/blob/master/lingvo/core/gpipe.py]
3. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding [[arXiv]https://arxiv.org/abs/2006.16668][[code]https://github.com/tensorflow/lingvo/tree/master/lingvo/tasks/lm]
4. 









